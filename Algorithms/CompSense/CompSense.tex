\lab{Algorithm}{Compressed Sensing}{Compressed Sensing}
\label{Ch:CS}

\objective{Teach about Compressed Sensing.}

\section*{Shannon-Nyquist}

If you want to reconstruct a signal perfectly how many samples do you need? According to the  Shannon-Nyquist thereom you need twice as many as your highest frequency.


\begin{problem}
Yea
\end{problem}


\section*{Compressed Sensing}


Compressed Sensing is a new technique for reconstructing sparse signals using a relatively small number of measurements. Specifically, Compressed Sensing allows for the exact reconstruction of a sparse $m-$dimensional signal with $k$ non-zero components with $O(k \log (m))$ measurements. This is a tremendous result, particularly because it improves upon the Shannon-Nyquist criteria \emph{exponentially}! This technique was first developed in 2005 and 2006, and since has been applied in a variety of fields, including image and video processing , MRI, machine learning and communications.

We will explain the basic idea behind Compressed Sensing using a simple example presented in [Hayes 2009]


\hangindent = .7cm
\setlength{\parindent}{.7cm}
\textbf{Counterfeit Coin Problem:} Given $2^n$ coins, with one lighter than the rest, how many measurements (on a balance) are needed to identify the lighter coin?
\setlength{\parindent}{0cm}

The answer, as you might have deduced, is $\log(n)$ (For example in the case of 8 coins you measure $\{1,2,3,4\}$, $\{1,2,5,6\}$ and $\{1,3,5,7\}$. In this setting it's quite obvious that individually measuring each coin is highly ineffective. You can probably further deduce that identifying $k$ lighter coins would take $O(k \log(n))$ measurements.

Several recent results say that we can reconstruct more general sparse signals using a similar technique. Specifically we have the following result:

\hangindent = .7cm
\setlength{\parindent}{.7cm}
\textbf{Compressed Sensing:} Suppose that we have $x \in \R^n$  sufficiently sparse and an $m \times n$ matrix $A$ $(m < n)$ satisfying certain properties (to be described later), with $Ax = b$. Then the solution of the optimization problem:
\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}}
& & \|x\|_0\\
& \text{subject to}
& & Ax = b.
\end{aligned}
\end{equation*}
 will yield $x$.
\setlength{\parindent}{0cm}

One important technical detail is the matrix $A$.  It must have what is called the \emph{Restricted Isometry Principle}. This property is rather technical, but it is generally satisfied by a random matrix of appropriate size. The ``randomness'' of the matrix is not very restrictive, and most random matrices based upon standard distributions satisfy this property, including bernoulli and gaussian.

We note that the signal need not be sparse in the domain in which we are measuring. The signal may be sparse in some appropriate transform domain (i.e. Fourier or wavelet). This makes this approach very appealing in a variety of applications, particularly to graphics and image processing. When the measurements are in a different basis than the matrix that the signal is sparse in than you can change the form to $ADx = b$. Where is $D$ is an $n$ by $n$ change of basis matrix and $A$ in a random matrix as defined above. Some basis, such as the fourier transformation, sastify the \emph{Restricted Isometry Principle}. So you can sample in the time domain and reconstuct the sparse signal in the fourier basis.

\section*{The $L_0$ norm}

The $L_0$ norm is defined as follows: For $x \in \mathbb{R}^n$, $\|x\|_0=\sum_{k=1}^n{x_k^0}$ or the number of non-zero elements to x. This problem is, in general, NP-Hard. However, the groundbreaking results  guarantee that, given a sufficiently sparse signal, with overwhelming probability we can replace $l_0$ with $l_1$ and acheive the same result. 
TO DO see picture.
The $l_1$ minimization problem is a convex problem, and thus is generally amicable to computation.

\section*{How to solve it and code examples}


\begin{problem}
How to, do a problem 
\end{problem}

\begin{problem}
Use this file. It is sparse.  
\end{problem}



