\lab{Algorithms}{An Introduction to Parallel Programming using MPI}{Introduction to Parallel Programming}
\objective{Learn the basics of parallel computing on distributed memory machines using MPI for Python}
\label{lab:MPI_Intro}

\section*{Why Parallel Computing?}
Over the past few decades, vast increases in computational power have come through
increased single processor performance, which have almost wholly been driven by smaller transistors.
However, these faster transistors generate more heat, which destabilizes circuits.
The so-called ``heat wall'' refers to the physical limits of single processors to
become faster because of the growing inability to sufficiently dissipate heat.

Parallel computing was invetened to circumvent this physical limitation.
It began, as with most things, in many different systems and styles.
Some systems had shared memory between many processors and some systems had a separate
program for each processor while others ran all the processors off the same base program.

Today, the most commonly used method for high performance computing is a single-program,
multiple-data system with message passing.
Essentially, this means that supercomputers are made up of many, many normal computers, each with their own memory.
These normal computers are all running the same program and are able to communicate with each other. Although they all run the same program file, each process takes a different execution path through the code, as a result of the interactions that message passing makes possible.

Note that the commands being executed by two different processes are going to be different.
Thus, we could actually write two separate computer programs in different files to accomplish this.
However, the most common method today has become to write both processes in a single program,
using conditionals and techniques from the Message Passing Interface to control which lines of code get executed by each process.

This is a very different architecture than ``normal'' computers, and it requires a
different kind of software. You can't take a traditional program and expect it to
magically run faster on a supercomputer; you must design new algorithms in ways that take advantage of the unique advantages of parallel computing.

\section*{MPI: the Message Passing Interface}
At its most basic, the Message Passing Interface (MPI) provides functions for sending
and receiving messages between different processes. You can think of a process as one of the many ``normal'' computers that make up a supercomputer. Actually,

MPI was developed out of the need for standardization of programming parallel systems.
It is different than other approaches in that MPI does not require programs to be written in a particular language.
Rather, MPI specifies a library of functions--the syntax and semantics of message passing routines--that can be called from other programming languages,
such as Python and C. MPI provides a very powerful and very general way of expressing parallelism.
It can be thought of as ``the assembly language of parallel computing,'' because of this
generality and the detail that it forces the programmer to deal with\footnote{Parallel Programming with MPI, by Peter S. Pacheco, p. 7}.
In spite of this apparent drawback, MPI is still very important. It was the first portable,
universally available standard for programming parallel systems, and it is the \emph{de facto} standard today.
MPI makes it possible for programmers to develop portable, parallel software libraries.
Until recently, one of the biggest problems in parallel computing was the lack of software.
However, parallel software is growing faster thanks in large part to this standardization.

% \begin{problem}
% Most modern personal computers now have multicore processors.
% In order to take advantage of the extra available computational power, a single program must be specially designed.
% Programs that are designed for these multicore processors are also ``parallel'' programs, typically written using POSIX threads or OpenMP.
% MPI, on the other hand, is designed with a different kind of architecture in mind.
% How does the architecture of a system for which MPI is designed differ what POSIX threads or OpenMP is designed for?
% What is the difference between MPI and OpenMP or Pthreads?
% \end{problem}
% TODO research this yourself and figure out whether multiple MPI processes can be run on a single computer

\section*{Why MPI for Python?}
In general, parallel programs are much more difficult and complex than ``normal'', serial programs.
Python is an excellent language for algorithm design and for solving problems that
don't require maximum performance.
This makes Python great for prototyping and writing small or medium-sized parallel programs.
However, Python is not designed specifically for high performance computing and its
parallel capabilities are still somewhat underdeveloped, so in practice it is better
to write production code in fast, compiled languages, such as C or Fortran.

We will use the Python library \texttt{mpi4py}. This package implements the Message Passing Interface in a basic way that makes it relatively easy to port your code to a faster language such as C for production code.

\section*{Introduction to MPI}
As tradition has it, we will start with a Hello World program.
\lstinputlisting[style=fromfile]{hello.py}
Save this program as \texttt{hello.py} and execute it from the command line as follows:
\begin{lstlisting}[style=ShellInput]
$ mpirun -n 5 python hello.py
\end{lstlisting}
\begin{info}
If the program fails to start, you may need to try the following syntax:
\begin{lstlisting}[style=ShellInput]
$ mpirun -n 5 python.exe hello.py
\end{lstlisting}
\end{info}
The program should output something like this:
\begin{lstlisting}[style=ShellOutput]
Hello world! I'm process number 3.
Hello world! I'm process number 2.
Hello world! I'm process number 0.
Hello world! I'm process number 4.
Hello world! I'm process number 1.
\end{lstlisting}
Notice that when you try this on your own, the lines will not necessarily print in order.
The five processes will run autonomously, so their output could be printed in any order.

\begin{warn}
Unless you are debugging, it is usually bad practice to perform I/O (e.g., call \li{print}) from any process
other than the root process (the \emph{root process} is the process with rank 0)
\end{warn}

\section*{Execution}
How does this program work? When we execute
\begin{lstlisting}[style=ShellInput]
$ mpirun -n 5 python hello.py
\end{lstlisting}
a number of things happen:

First, the \texttt{mpirun} program is launched.
This is the program which starts MPI, a wrapper around whatever program you to pass into it.
The ``-n 5'' option specifies the desired number of processes.
In this case, 5 processes are initialized.
The rest of this command, ``python hello.py'', is the command that each process will run.
Thus, each of the five processes will run a copy of \texttt{hello.py} in its own execution environment, separate from the other 4 processes.
These 5 execution environments are identical except for one key difference: Each process is assigned a different ``rank'' within the \texttt{mpirun} program.
Because of this, each process prints a different number when it executes.

What are ranks and how do they work? When \texttt{mpirun} is first executed, it creates a global communicator object. Each process can access can access this object through the variable \li{MPI.COMM_WORLD} within the Python program.
One of the main purposes of this communicator is to keep track of each process's ``rank'', which is a unique identifier.

When each process executes the line \li{RANK = COMM.Get_rank()}, the process queries the global communicator to figure out what the process's ``rank'' is.
Process ranks are essential to interprocess communication and coordination because they are the only difference between each process's otherwise identical execution environment.

After each process executes the line \li{RANK = COMM.Get_rank()}, each process's local variable \li{RANK} will be a unique from each of the other 4 processes.
This gives us a way to distinguish between different processes while allowing us to write the source code
for the 5 processes in a single file.

\section*{MPI Communicators}
An MPI communicator object basically just represents a collection of processes.
Although processes can access communicators through the interface MPI provides, communicators' data is stored within the execution environment of the \texttt{mpirun} environment, not within the execution environments of the individual processes.

Communicators allow processes to send and receive messages.
In most of our programs we will only deal with the \li{MPI.COMM_WORLD} communicator, which contains all of the running processes.

\begin{info}
In more advanced MPI programs, it is possible to create custom communicators that group certain subsets of the processes together. This can be useful (especially in supercomputers) because MPI can physically rearrange which processes are assigned to which CPUs, thereby optimizing your program's speed by making sure that the processes within each communicator can quickly communicate with each other.

Note that a single process can be a member of many different communicators, and that its rank will most likely be different in each communicator. However, since our programs will just be using the global communicator \li{MPI.COMM_WORLD}, we can think of ranks as being unique IDs assigned to each process.
\end{info}
% TODO add picture here of execution environments. (Or possibly somewhere above here)

Another very basic method of communicators is \li{COMM.Get_size}, which returns the number of processes that exist within the communicator.
Take the following program as an example:
\lstinputlisting[style=fromfile]{getSize.py}
Now, run this program from the command line using this command:
\begin{lstlisting}[style=ShellInput]
$ mpirun -n 3 python getSize.py
\end{lstlisting}
The program should output this:
\begin{lstlisting}[style=ShellOutput]
There are 3 processes running.
There are 3 processes running.
There are 3 processes running.
\end{lstlisting}

\begin{problem}
Write a program in which the the processes with an even rank print ``Hello'' and process with an odd rank print ``Goodbye''.
Additionally, each process should print its rank and the total number of processes currently running.
This program should work for any number of processes.
For example, if you run this program with 6 processes, process 3 should print ``Goodbye from process 3 out of 6''.
\end{problem}

\begin{problem}
Sometimes, a parallel algorithm can only run correctly if it has a certain number of processes.
Although you typically want to avoid writing these kinds of programs, sometimes it is
inconvenient or unavoidable. Write a program that runs only if it has exactly 5 processes.
Upon failure, the root process should print ``Error: This program must run with exactly 5 processes''
and upon success \emph{every} process should print ``Success!'' (To exit, call the function
\li{COMM.Abort()} from the root process)
\end{problem}
